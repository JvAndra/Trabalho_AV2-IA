{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "QCr5i1kqMHvi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.utils import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CONFIGURA√á√ïES ---\n",
        "hp = {\n",
        "    'MAX_WORD_INDEX': 1000,\n",
        "    'MAX_LEN': 300,       # Sequ√™ncias inicial (30)\n",
        "    'N_STATES': 3 ,       # N\n",
        "    'N_ITER': 30          # Itera√ß√µes iniciais (5)\n",
        "}\n",
        "\n",
        "print(\"1. Carregando dados...\")\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=hp['MAX_WORD_INDEX'])\n",
        "\n",
        "# Separamos os dados por classe para treinar os HMM\n",
        "# Modelo Positivo s√≥ v√™ reviews positivos (label 1)\n",
        "# Modelo Negativo s√≥ v√™ reviews negativos (label 0)\n",
        "train_pos = [x for x, y in zip(train_data, train_labels) if y == 1]\n",
        "train_neg = [x for x, y in zip(train_data, train_labels) if y == 0]\n",
        "\n",
        "# Quantidade para o treino\n",
        "LIMIT_TRAIN = 300\n",
        "train_pos = pad_sequences(train_pos[:LIMIT_TRAIN], maxlen=hp['MAX_LEN'])\n",
        "train_neg = pad_sequences(train_neg[:LIMIT_TRAIN], maxlen=hp['MAX_LEN'])\n",
        "X_test = pad_sequences(test_data[:100], maxlen=hp['MAX_LEN']) # Teste pequeno\n",
        "y_test_sample = test_labels[:100]\n",
        "\n",
        "# --- CLASSE HMM ---\n",
        "\n",
        "class HMMManual:\n",
        "    def __init__(self, n_states, n_emissions):\n",
        "        self.N = n_states\n",
        "        self.M = n_emissions\n",
        "\n",
        "        # Inicializa√ß√£o aleat√≥ria e normalizada\n",
        "        np.random.seed(42)\n",
        "        self.pi = np.random.rand(self.N)\n",
        "        self.pi /= np.sum(self.pi)\n",
        "\n",
        "        self.A = np.random.rand(self.N, self.N)\n",
        "        self.A /= self.A.sum(axis=1, keepdims=True)\n",
        "\n",
        "        self.B = np.random.rand(self.N, self.M)\n",
        "        self.B /= self.B.sum(axis=1, keepdims=True)\n",
        "\n",
        "    def _forward(self, obs):\n",
        "        \"\"\"Calcula alpha (probabilidade de chegar no estado i no tempo t)\"\"\"\n",
        "        T = len(obs)\n",
        "        alpha = np.zeros((T, self.N))\n",
        "        scale = np.zeros(T) # Fator de escala para evitar underflow\n",
        "\n",
        "        # Inicializa√ß√£o\n",
        "        alpha[0] = self.pi * self.B[:, obs[0]]\n",
        "        scale[0] = np.sum(alpha[0]) + 1e-10 # +1e-10 evita divis√£o por zero\n",
        "        alpha[0] /= scale[0]\n",
        "\n",
        "        # Indu√ß√£o (indo)\n",
        "        for t in range(1, T):\n",
        "            for j in range(self.N):\n",
        "                alpha[t, j] = np.sum(alpha[t-1] * self.A[:, j]) * self.B[j, obs[t]]\n",
        "\n",
        "            scale[t] = np.sum(alpha[t]) + 1e-10\n",
        "            alpha[t] /= scale[t]\n",
        "\n",
        "        return alpha, scale\n",
        "\n",
        "    def _backward(self, obs, scale):\n",
        "        \"\"\"Calcula beta (probabilidade do futuro dado o estado i no tempo t)\"\"\"\n",
        "        T = len(obs)\n",
        "        beta = np.zeros((T, self.N))\n",
        "\n",
        "        # Inicializa√ß√£o (no tempo T, beta = 1)\n",
        "        beta[T-1] = 1.0 / scale[T-1] # Usa a mesma escala do forward\n",
        "\n",
        "        # Indu√ß√£o Reversa (vindo)\n",
        "        for t in range(T-2, -1, -1):\n",
        "            for i in range(self.N):\n",
        "                # transi√ß√£o * emiss√£o_futura * beta_futuro\n",
        "                beta[t, i] = np.sum(self.A[i, :] * self.B[:, obs[t+1]] * beta[t+1, :])\n",
        "\n",
        "            beta[t] /= scale[t] # Normaliza com escala do forward\n",
        "\n",
        "        return beta\n",
        "\n",
        "    def train(self, sequences, n_iter=5):\n",
        "        for it in range(n_iter):\n",
        "            # A_num[i, j] = Esperan√ßa de transi√ß√µes de i para j\n",
        "            A_num = np.zeros((self.N, self.N))\n",
        "            A_den = np.zeros((self.N, 1))\n",
        "\n",
        "            # B_num[j, k] = Esperan√ßa de estar em j e emitir k\n",
        "            B_num = np.zeros((self.N, self.M))\n",
        "            B_den = np.zeros((self.N, 1))\n",
        "\n",
        "            Pi_new = np.zeros(self.N)\n",
        "\n",
        "            # E-STEP: Calcula estat√≠sticas para cada sequ√™ncia\n",
        "            for seq in sequences:\n",
        "                # Remove padding (0) para o treino\n",
        "                obs = [x for x in seq if x != 0]\n",
        "                if len(obs) < 2: continue\n",
        "\n",
        "                T = len(obs)\n",
        "                alpha, scale = self._forward(obs)\n",
        "                beta = self._backward(obs, scale)\n",
        "\n",
        "                # Calcula Gamma (Prob de estar no estado i no tempo t)\n",
        "                # gamma[t, i] ~ alpha * beta\n",
        "                gamma = alpha * beta\n",
        "                # Normaliza gamma\n",
        "                gamma /= (np.sum(gamma, axis=1, keepdims=True) + 1e-10)\n",
        "\n",
        "                # Calcula Xi (Prob de transi√ß√£o i -> j no tempo t)\n",
        "                xi = np.zeros((T-1, self.N, self.N))\n",
        "                for t in range(T-1):\n",
        "                    denom = np.sum(alpha[t,:] * np.sum(self.A * self.B[:, obs[t+1]] * beta[t+1, :], axis=1)) + 1e-10\n",
        "                    for i in range(self.N):\n",
        "                        xi[t, i, :] = alpha[t, i] * self.A[i, :] * self.B[:, obs[t+1]] * beta[t+1, :] / denom\n",
        "\n",
        "                # M-STEP (Acumula√ß√£o): Soma as ocorr√™ncias esperadas\n",
        "                Pi_new += gamma[0]\n",
        "\n",
        "                # Atualiza acumuladores de A\n",
        "                A_num += np.sum(xi, axis=0)\n",
        "                A_den += np.sum(gamma[:-1], axis=0).reshape(-1, 1)\n",
        "\n",
        "                # Atualiza acumuladores de B\n",
        "                for t in range(T):\n",
        "                    symbol = obs[t]\n",
        "                    B_num[:, symbol] += gamma[t]\n",
        "                B_den += np.sum(gamma, axis=0).reshape(-1, 1)\n",
        "\n",
        "            # ATUALIZA√á√ÉO FINAL DOS PAR√ÇMETROS\n",
        "            self.pi = Pi_new / len(sequences)\n",
        "            self.A = A_num / (A_den + 1e-10) # +1e-10 evita div por 0\n",
        "            self.B = B_num / (B_den + 1e-10)\n",
        "\n",
        "            print(f\"  > Itera√ß√£o {it+1}/{n_iter} conclu√≠da.\")\n",
        "\n",
        "    def score(self, observation_sequence):\n",
        "        \"\"\"Retorna Log-Likelihood (usado para classificar)\"\"\"\n",
        "        obs = [o for o in observation_sequence if o != 0]\n",
        "        if len(obs) == 0: return -np.inf\n",
        "\n",
        "        _, scale = self._forward(obs)\n",
        "        # Log da probabilidade √© a soma dos logs das escalas\n",
        "        return np.sum(np.log(scale + 1e-10))\n",
        "\n",
        "# --- TRAINING LOOP ---\n",
        "\n",
        "print(\"\\n2. Treinando Modelo POSITIVO (Isso pode demorar um pouco)...\")\n",
        "hmm_pos = HMMManual(hp['N_STATES'], hp['MAX_WORD_INDEX'] + 1)\n",
        "hmm_pos.train(train_pos, n_iter=hp['N_ITER'])\n",
        "\n",
        "print(\"\\n3. Treinando Modelo NEGATIVO...\")\n",
        "hmm_neg = HMMManual(hp['N_STATES'], hp['MAX_WORD_INDEX'] + 1)\n",
        "hmm_neg.train(train_neg, n_iter=hp['N_ITER'])\n",
        "\n",
        "print(\"\\n4. Testando Acur√°cia...\")\n",
        "acertos = 0\n",
        "for i in range(len(X_test)):\n",
        "    seq = X_test[i]\n",
        "    if len([x for x in seq if x != 0]) < 2: continue\n",
        "\n",
        "    # Classifica√ß√£o Bayesiana (De quem √© mais prov√°vel ser essa frase)\n",
        "    score_pos = hmm_pos.score(seq)\n",
        "    score_neg = hmm_neg.score(seq)\n",
        "\n",
        "    pred = 1.0 if score_pos > score_neg else 0.0\n",
        "\n",
        "    if pred == y_test_sample[i]:\n",
        "        acertos += 1\n",
        "\n",
        "print(f\"\\n=== RESULTADO FINAL ===\")\n",
        "print(f\"Acur√°cia no teste ({len(X_test)} amostras): {acertos/len(X_test):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r83tv38NMT7N",
        "outputId": "efec2cdf-460e-47a1-c883-4c30bb57fb82"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Carregando dados...\n",
            "\n",
            "2. Treinando Modelo POSITIVO (Isso pode demorar um pouco)...\n",
            "  > Itera√ß√£o 1/30 conclu√≠da.\n",
            "  > Itera√ß√£o 2/30 conclu√≠da.\n",
            "  > Itera√ß√£o 3/30 conclu√≠da.\n",
            "  > Itera√ß√£o 4/30 conclu√≠da.\n",
            "  > Itera√ß√£o 5/30 conclu√≠da.\n",
            "  > Itera√ß√£o 6/30 conclu√≠da.\n",
            "  > Itera√ß√£o 7/30 conclu√≠da.\n",
            "  > Itera√ß√£o 8/30 conclu√≠da.\n",
            "  > Itera√ß√£o 9/30 conclu√≠da.\n",
            "  > Itera√ß√£o 10/30 conclu√≠da.\n",
            "  > Itera√ß√£o 11/30 conclu√≠da.\n",
            "  > Itera√ß√£o 12/30 conclu√≠da.\n",
            "  > Itera√ß√£o 13/30 conclu√≠da.\n",
            "  > Itera√ß√£o 14/30 conclu√≠da.\n",
            "  > Itera√ß√£o 15/30 conclu√≠da.\n",
            "  > Itera√ß√£o 16/30 conclu√≠da.\n",
            "  > Itera√ß√£o 17/30 conclu√≠da.\n",
            "  > Itera√ß√£o 18/30 conclu√≠da.\n",
            "  > Itera√ß√£o 19/30 conclu√≠da.\n",
            "  > Itera√ß√£o 20/30 conclu√≠da.\n",
            "  > Itera√ß√£o 21/30 conclu√≠da.\n",
            "  > Itera√ß√£o 22/30 conclu√≠da.\n",
            "  > Itera√ß√£o 23/30 conclu√≠da.\n",
            "  > Itera√ß√£o 24/30 conclu√≠da.\n",
            "  > Itera√ß√£o 25/30 conclu√≠da.\n",
            "  > Itera√ß√£o 26/30 conclu√≠da.\n",
            "  > Itera√ß√£o 27/30 conclu√≠da.\n",
            "  > Itera√ß√£o 28/30 conclu√≠da.\n",
            "  > Itera√ß√£o 29/30 conclu√≠da.\n",
            "  > Itera√ß√£o 30/30 conclu√≠da.\n",
            "\n",
            "3. Treinando Modelo NEGATIVO...\n",
            "  > Itera√ß√£o 1/30 conclu√≠da.\n",
            "  > Itera√ß√£o 2/30 conclu√≠da.\n",
            "  > Itera√ß√£o 3/30 conclu√≠da.\n",
            "  > Itera√ß√£o 4/30 conclu√≠da.\n",
            "  > Itera√ß√£o 5/30 conclu√≠da.\n",
            "  > Itera√ß√£o 6/30 conclu√≠da.\n",
            "  > Itera√ß√£o 7/30 conclu√≠da.\n",
            "  > Itera√ß√£o 8/30 conclu√≠da.\n",
            "  > Itera√ß√£o 9/30 conclu√≠da.\n",
            "  > Itera√ß√£o 10/30 conclu√≠da.\n",
            "  > Itera√ß√£o 11/30 conclu√≠da.\n",
            "  > Itera√ß√£o 12/30 conclu√≠da.\n",
            "  > Itera√ß√£o 13/30 conclu√≠da.\n",
            "  > Itera√ß√£o 14/30 conclu√≠da.\n",
            "  > Itera√ß√£o 15/30 conclu√≠da.\n",
            "  > Itera√ß√£o 16/30 conclu√≠da.\n",
            "  > Itera√ß√£o 17/30 conclu√≠da.\n",
            "  > Itera√ß√£o 18/30 conclu√≠da.\n",
            "  > Itera√ß√£o 19/30 conclu√≠da.\n",
            "  > Itera√ß√£o 20/30 conclu√≠da.\n",
            "  > Itera√ß√£o 21/30 conclu√≠da.\n",
            "  > Itera√ß√£o 22/30 conclu√≠da.\n",
            "  > Itera√ß√£o 23/30 conclu√≠da.\n",
            "  > Itera√ß√£o 24/30 conclu√≠da.\n",
            "  > Itera√ß√£o 25/30 conclu√≠da.\n",
            "  > Itera√ß√£o 26/30 conclu√≠da.\n",
            "  > Itera√ß√£o 27/30 conclu√≠da.\n",
            "  > Itera√ß√£o 28/30 conclu√≠da.\n",
            "  > Itera√ß√£o 29/30 conclu√≠da.\n",
            "  > Itera√ß√£o 30/30 conclu√≠da.\n",
            "\n",
            "4. Testando Acur√°cia...\n",
            "\n",
            "=== RESULTADO FINAL ===\n",
            "Acur√°cia no teste (100 amostras): 0.84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Teste ---\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "# Garantir que usamos o mesmo mapeamento que o Keras usou no treino\n",
        "word_index = imdb.get_word_index()\n",
        "# O Keras reserva os primeiros √≠ndices: 0=Padding, 1=Start, 2=Unknown\n",
        "word_index = {k: (v + 3) for k, v in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2\n",
        "\n",
        "def classificar_frase(texto):\n",
        "    # Limpeza\n",
        "    texto_limpo = texto.lower().replace('.', '').replace(',', '').replace('!', '').replace('?', '')\n",
        "    palavras = texto_limpo.split()\n",
        "\n",
        "    # Tradu√ß√£o\n",
        "    seq_numerica = []\n",
        "    for w in palavras:\n",
        "        if w in word_index:\n",
        "            idx = word_index[w]\n",
        "            if idx >= hp['MAX_WORD_INDEX']:\n",
        "                seq_numerica.append(2) # 2 = Desconhecido\n",
        "            else:\n",
        "                seq_numerica.append(idx)\n",
        "        else:\n",
        "            seq_numerica.append(2) # Palavra que n√£o existe no IMDB vira <UNK>\n",
        "\n",
        "    # Formata√ß√£o para o HMM\n",
        "    seq_formatada = np.array([x for x in seq_numerica if x != 0])\n",
        "\n",
        "    if len(seq_formatada) == 0:\n",
        "        return \"Frase vazia ou palavras fora do vocabul√°rio conhecido.\"\n",
        "\n",
        "    # Bayes (Score nos dois modelos)\n",
        "    log_prob_pos = hmm_pos.score(seq_formatada) # Modelo Positivo\n",
        "    log_prob_neg = hmm_neg.score(seq_formatada) # Modelo Negativo\n",
        "\n",
        "    # Decis√£o\n",
        "    print(f\"Frase: \\\"{texto}\\\"\")\n",
        "    print(f\"Score Positivo: {log_prob_pos:.2f}\")\n",
        "    print(f\"Score Negativo: {log_prob_neg:.2f}\")\n",
        "\n",
        "    if log_prob_pos > log_prob_neg:\n",
        "        return \">> RESULTADO: üòÑ POSITIVO\"\n",
        "    else:\n",
        "        return \">> RESULTADO: üò° NEGATIVO\"\n",
        "\n",
        "\n",
        "\n",
        "frase_teste = \"I hated this movie it is the worst film ever terrible story\"\n",
        "\n",
        "# Executa a classifica√ß√£o\n",
        "print(classificar_frase(frase_teste))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Km-39d8JA6Yf",
        "outputId": "05acf6cc-7373-4362-a108-a812ec017dc6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frase: \"I hated this movie it is the worst film ever terrible story\"\n",
            "Score Positivo: -66.62\n",
            "Score Negativo: -62.21\n",
            ">> RESULTADO: üò° NEGATIVO\n"
          ]
        }
      ]
    }
  ]
}