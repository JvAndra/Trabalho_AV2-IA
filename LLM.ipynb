{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch tensorflow --quiet\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import mixed_precision\n",
        "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "\n",
        "hp = {\n",
        "    'MAX_WORD_INDEX': 20000,\n",
        "    'MAX_LEN': 128,\n",
        "    'BATCH_SIZE': 32,\n",
        "    'EPOCHS': 3,\n",
        "    'LEARNING_RATE': 0.00002,\n",
        "    'MAX_SAMPLES': 2000,\n",
        "    'MAX_TEST_SAMPLES': 750\n",
        "}\n",
        "\n",
        "\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "word_index = {k: (v + 3) for k, v in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2\n",
        "reverse_word_index = {v: k for k, v in word_index.items()}\n",
        "\n",
        "def decode_review(text_ids):\n",
        "    return ' '.join([reverse_word_index.get(i, '') for i in text_ids])\n",
        "\n",
        "(train_sequences, train_labels), (test_sequences, test_labels) = imdb.load_data(num_words=hp['MAX_WORD_INDEX'])\n",
        "\n",
        "if hp['MAX_SAMPLES'] is not None:\n",
        "    n = min(hp['MAX_SAMPLES'], len(train_sequences))\n",
        "    train_sequences = train_sequences[:n]\n",
        "    train_labels = train_labels[:n]\n",
        "\n",
        "if hp['MAX_TEST_SAMPLES'] is not None:\n",
        "    m = min(hp['MAX_TEST_SAMPLES'], len(test_sequences))\n",
        "    test_sequences = test_sequences[:m]\n",
        "    test_labels = test_labels[:m]\n",
        "\n",
        "\n",
        "train_textos = [decode_review(x) for x in train_sequences]\n",
        "test_textos = [decode_review(x) for x in test_sequences]\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def bert_encode(textos, max_len=hp['MAX_LEN']):\n",
        "    return tokenizer(\n",
        "        textos,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_len,\n",
        "        return_tensors=\"tf\"\n",
        "    )\n",
        "\n",
        "train_encodings = bert_encode(train_textos, hp['MAX_LEN'])\n",
        "test_encodings = bert_encode(test_textos, hp['MAX_LEN'])\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels))\n",
        "train_ds = train_ds.shuffle(1000).batch(hp['BATCH_SIZE']).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_labels))\n",
        "test_ds = test_ds.batch(hp['BATCH_SIZE']).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "try:\n",
        "    model = TFDistilBertForSequenceClassification.from_pretrained(\n",
        "        \"distilbert-base-uncased\",\n",
        "        num_labels=2,\n",
        "        use_safetensors=False\n",
        "    )\n",
        "except:\n",
        "    model = TFDistilBertForSequenceClassification.from_pretrained(\n",
        "        \"distilbert-base-uncased\",\n",
        "        num_labels=2,\n",
        "        from_pt=True\n",
        "    )\n",
        "\n",
        "\n",
        "for layer in model.distilbert.transformer.layer[:3]:\n",
        "    layer.trainable = False\n",
        "\n",
        "original_policy = mixed_precision.global_policy()\n",
        "mixed_precision.set_global_policy(mixed_precision.Policy('float32'))\n",
        "\n",
        "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=hp['LEARNING_RATE'])\n",
        "\n",
        "mixed_precision.set_global_policy(original_policy)\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy(\"accuracy\")\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss_fn, metrics=[metric])\n",
        "\n",
        "early_stop = keras.callbacks.EarlyStopping(\n",
        "    patience=1,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=test_ds,\n",
        "    epochs=hp['EPOCHS'],\n",
        "    callbacks=[early_stop]\n",
        ")\n",
        "\n",
        "loss, acc = model.evaluate(test_ds)\n",
        "\n",
        "def classificar_frase(texto):\n",
        "    enc = tokenizer(texto, return_tensors=\"tf\", truncation=True, padding=True, max_length=hp['MAX_LEN'])\n",
        "    logits = model(enc).logits\n",
        "    probs = tf.nn.softmax(logits)[0].numpy()\n",
        "    rotulo = \"POSITIVO\" if np.argmax(probs) == 1 else \"NEGATIVO\"\n",
        "    return rotulo, float(np.max(probs))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Kh7SS7bAnpkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frase_teste = \"The movie was okay at the beginning but the ending ruined everything.\"\n",
        "lbl, conf = classificar_frase(frase_teste)\n",
        "print(f\"\\nTeste Manual: \\\"{frase_teste}\\\" → {lbl} ({conf:.2f})\")\n",
        "\n",
        "frase_teste = \"Men want to give birth sooooo bad it’s crazy\"\n",
        "lbl, conf = classificar_frase(frase_teste)\n",
        "print(f\"\\nTeste Manual: \\\"{frase_teste}\\\" → {lbl} ({conf:.2f})\")"
      ],
      "metadata": {
        "id": "VPao74Vhq63a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}